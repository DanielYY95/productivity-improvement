import os
import json
import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

# ================= CONFIGURATION =================
# Obsidian Vaultì˜ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì • (.envì—ì„œ ë¡œë“œ)
env_vault_path = os.getenv("VAULT_PATH")
if not env_vault_path:
    # .envì— ì„¤ì •ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ê¸°ë³¸ê°’ (í•„ìš”ì‹œ ìˆ˜ì •)
    raise ValueError("VAULT_PATH is not set in .env file.")
else:
    VAULT_PATH = Path(env_vault_path)

today_str = datetime.datetime.now().strftime("%Y.%m.%d")

# ì…ë ¥ íŒŒì¼: ë§¤ì¼ ì‘ì„±í•˜ëŠ” ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (Inbox)
# .envì—ì„œ í´ë”ëª…ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ "daily" ì‚¬ìš©
INBOX_DIR_NAME = os.getenv("INBOX_DIR_NAME", "daily")
INBOX_FILE = VAULT_PATH / INBOX_DIR_NAME / f"{today_str}.md"

# ì¶œë ¥ í´ë”: ì£¼ì œë³„ë¡œ ë¶„ë¥˜ëœ ìš”ì•½ íŒŒì¼ì´ ì €ì¥ë  ê²½ë¡œ (Knowledge)
# .envì—ì„œ í´ë”ëª…ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ "summary" ì‚¬ìš©
KNOWLEDGE_DIR_NAME = os.getenv("KNOWLEDGE_DIR_NAME", "summary")
KNOWLEDGE_DIR = VAULT_PATH / KNOWLEDGE_DIR_NAME

# ì•„ì¹´ì´ë¸Œ íŒŒì¼: ì²˜ë¦¬ê°€ ì™„ë£Œëœ ì›ë³¸ ë¡œê·¸ë¥¼ ë°±ì—…í•  íŒŒì¼ ê²½ë¡œ
# .envì—ì„œ í´ë”ëª…ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ "daily_archive" ì‚¬ìš©
ARCHIVE_DIR_NAME = os.getenv("ARCHIVE_DIR_NAME", "daily_archive")
ARCHIVE_FILE = VAULT_PATH / ARCHIVE_DIR_NAME / f"{today_str}_done.md"

# ì‚¬ìš©í•  AI ëª¨ë¸ ì´ë¦„ (.envì—ì„œ ì„¤ì • ê°€ëŠ¥, ê¸°ë³¸ê°’: qwen2.5:32b)
MODEL_NAME = os.getenv("MODEL_NAME", "qwen2.5:32b")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Ollama ë¡œì»¬ ì„œë²„ ë˜ëŠ” OpenAI API ì‚¬ìš©)
client = OpenAI(
    base_url=os.getenv("OPENAI_BASE_URL", "http://localhost:11434/v1"),
    api_key=os.getenv("OPENAI_API_KEY", "ollama"),
)
# =================================================


class ObsidianBatchProcessor:
    def __init__(self) -> None:
        self.today = datetime.datetime.now().strftime("%Y-%m-%d")

    def read_inbox(self) -> Optional[str]:
        if not INBOX_FILE.exists():
            print(f"Error: inbox missing at {INBOX_FILE}")
            return None
        text = INBOX_FILE.read_text(encoding="utf-8").strip()
        return text if text else None

    def analyze_content(self, content: str) -> List[Dict[str, Any]]:
        prompt = f"""
Analyze the following text from my daily dev log.
The log may contain multiple distinct technical topics. Please identify and separate them.

Input Text:
{content}

Requirements:
1. Return ONLY a valid JSON array.
2. Identify ALL distinct technical topics discussed in the text. Do not merge unrelated topics (e.g., separate 'Docker Networking' from 'Java Runtime' if they are distinct sections).
3. Format: [{{"topic": "TopicName", "summary": "Korean summary...", "keywords": ["tag1", "tag2"]}}]
4. TopicName should be concise and specific (e.g., "Docker_Volume_Shadowing", "Traefik_vs_Nginx", "Java_vs_Node_Runtime").
5. Summary must be in Korean. Use Markdown formatting (bullet points, bold text) for better readability. Include key takeaways, solution steps, and comparisons if present.
6. Extract 3-5 important technical keywords for each topic.
7. If no meaningful technical content, return [].
"""
        try:
            resp = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "You are a technical documentation assistant. Respond in JSON only."},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.3,
                extra_body={"options": {"num_ctx": 16384}} # Ollama context window í™•ì¥
            )
            result_text = resp.choices[0].message.content or ""
            cleaned = result_text.replace("```json", "").replace("```", "").strip()
            return json.loads(cleaned)
        except Exception as e:
            print(f"API error: {e}")
            return []

    def append_to_topic_files(self, data: List[Dict[str, Any]]) -> None:
        KNOWLEDGE_DIR.mkdir(parents=True, exist_ok=True)
        for item in data:
            topic = (item.get("topic") or "Uncategorized").replace("/", "_")
            summary = item.get("summary") or ""
            keywords = item.get("keywords") or []

            path = KNOWLEDGE_DIR / f"{topic}.md"
            if not path.exists():
                path.write_text(f"# {topic}\n\nRunning Logs\n---\n", encoding="utf-8")
                print(f"Created new topic file: {path.name}")

            keywords_line = f"**Keywords**: {', '.join(f'`{k}`' for k in keywords)}" if keywords else ""
            append_text = f"\n### ğŸ“… {self.today} Summary\n{keywords_line}\n\n{summary}\n"

            with path.open("a", encoding="utf-8") as f:
                f.write(append_text)
            print(f"Appended to {path.name}")

    def archive_and_clear(self, original_content: str) -> None:
        ARCHIVE_FILE.parent.mkdir(parents=True, exist_ok=True)
        with ARCHIVE_FILE.open("a", encoding="utf-8") as f:
            f.write(f"\n## Processed on {self.today}\n{original_content}\n\n---\n")
        INBOX_FILE.write_text("", encoding="utf-8")
        print("âœ… Inbox archived and cleared.")

    def run(self) -> None:
        content = self.read_inbox()
        if not content:
            print("ğŸ“­ Inbox is empty. Skipping.")
            return
        data = self.analyze_content(content)
        if data:
            self.append_to_topic_files(data)
            self.archive_and_clear(content)
            print("ğŸ‰ Batch processing completed successfully.")
        else:
            print("âš ï¸ No valid data extracted from AI.")


if __name__ == "__main__":
    ObsidianBatchProcessor().run()
